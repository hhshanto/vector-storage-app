{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Embedding Models from Hugging Face\n",
    "\n",
    "1. 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "   - A compact and efficient model for generating sentence embeddings\n",
    "   - Good balance between performance and speed\n",
    "   - Suitable for various NLP tasks\n",
    "\n",
    "2. 'sentence-transformers/all-mpnet-base-v2'\n",
    "   - High-performance model for sentence embeddings\n",
    "   - Generally outperforms BERT-based models\n",
    "   - Excellent for semantic similarity tasks\n",
    "\n",
    "3. 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "   - Multilingual model supporting 50+ languages\n",
    "   - Good for cross-lingual tasks and multilingual datasets\n",
    "\n",
    "4. 'sentence-transformers/distilbert-base-nli-stsb-mean-tokens'\n",
    "   - DistilBERT-based model fine-tuned on NLI and STS datasets\n",
    "   - Faster than BERT while maintaining good performance\n",
    "   - Suitable for semantic similarity and clustering tasks\n",
    "\n",
    "5. 'openai-gpt'\n",
    "   - OpenAI's GPT model\n",
    "   - Good for general-purpose text embeddings\n",
    "   - Captures complex language patterns\n",
    "\n",
    "6. 'bert-base-uncased'\n",
    "   - Classic BERT model\n",
    "   - Widely used and well-understood\n",
    "   - Good baseline for many NLP tasks\n",
    "\n",
    "7. 'roberta-base'\n",
    "   - Improved version of BERT\n",
    "   - Often outperforms BERT on various benchmarks\n",
    "   - Excellent for a wide range of NLP tasks\n",
    "\n",
    "8. 'xlm-roberta-base'\n",
    "   - Multilingual version of RoBERTa\n",
    "   - Supports 100 languages\n",
    "   - Great for cross-lingual tasks\n",
    "\n",
    "9. 'allenai/scibert_scivocab_uncased'\n",
    "   - Specialized BERT model for scientific text\n",
    "   - Trained on a large corpus of scientific publications\n",
    "   - Ideal for scientific or technical domains\n",
    "\n",
    "10. 'microsoft/deberta-base'\n",
    "    - Enhanced BERT model with disentangled attention mechanism\n",
    "    - Strong performance on various NLP benchmarks\n",
    "    - Good for tasks requiring nuanced understanding of text\n",
    "\n",
    "# Usage example:\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def get_embedding(text, model_name='sentence-transformers/all-MiniLM-L6-v2'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "## Example:\n",
    "text = \"This is a sample sentence for embedding.\"\n",
    "embedding = get_embedding(text)\n",
    "print(f\"Embedding shape: {embedding.shape}\")\n",
    "\n",
    "We can easily switch models by changing the model_name parameter:\n",
    " embedding = get_embedding(text, model_name='roberta-base')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
