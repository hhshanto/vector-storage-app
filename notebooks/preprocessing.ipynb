{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Techniques for Vector Storage and Embedding\n",
    "\n",
    "This notebook explains the preprocessing techniques used in our project and why they are necessary for effective vector storage and embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text Cleaning\n",
    "\n",
    "### Technique:\n",
    "- Convert text to lowercase\n",
    "- Remove special characters and digits\n",
    "\n",
    "### Why it's necessary:\n",
    "- Consistency: Ensures that words like \"Hello\" and \"hello\" are treated the same.\n",
    "- Noise reduction: Removes irrelevant characters that could interfere with embedding.\n",
    "- Simplification: Makes the text easier to process in subsequent steps.\n",
    "\n",
    "Example implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Example\n",
    "original = \"Hello, World! 123\"\n",
    "cleaned = clean_text(original)\n",
    "print(f\"Original: {original}\")\n",
    "print(f\"Cleaned: {cleaned}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "### Technique:\n",
    "- Split text into individual words or subwords\n",
    "\n",
    "### Why it's necessary:\n",
    "- Granularity: Allows processing at the word level, which is crucial for most NLP tasks.\n",
    "- Preparation for embedding: Many embedding techniques work with individual tokens.\n",
    "- Enables further processing: Steps like stopword removal and lemmatization operate on tokens.\n",
    "\n",
    "Example implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Example\n",
    "text = \"This is a sample sentence.\"\n",
    "tokens = tokenize(text)\n",
    "print(f\"Original: {text}\")\n",
    "print(f\"Tokenized: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stopword Removal\n",
    "\n",
    "### Technique:\n",
    "- Remove common words that typically don't carry significant meaning\n",
    "\n",
    "### Why it's necessary:\n",
    "- Noise reduction: Removes words that don't contribute much to the overall meaning.\n",
    "- Efficiency: Reduces the number of tokens to process, potentially speeding up computations.\n",
    "- Focus: Allows the embedding to focus on more meaningful words.\n",
    "\n",
    "Example implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# Example\n",
    "tokens = [\"this\", \"is\", \"a\", \"sample\", \"sentence\"]\n",
    "filtered_tokens = remove_stopwords(tokens)\n",
    "print(f\"Original: {tokens}\")\n",
    "print(f\"Without stopwords: {filtered_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Lemmatization\n",
    "\n",
    "### Technique:\n",
    "- Reduce words to their base or dictionary form\n",
    "\n",
    "### Why it's necessary:\n",
    "- Normalization: Ensures different forms of a word (e.g., \"run\", \"running\", \"ran\") are treated as the same concept.\n",
    "- Vocabulary reduction: Reduces the number of unique tokens, which can be beneficial for some embedding techniques.\n",
    "- Consistency: Helps in maintaining consistency across different tenses and forms of words.\n",
    "\n",
    "Example implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Example\n",
    "tokens = [\"running\", \"cats\", \"better\", \"goes\"]\n",
    "lemmatized = lemmatize(tokens)\n",
    "print(f\"Original: {tokens}\")\n",
    "print(f\"Lemmatized: {lemmatized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "These preprocessing techniques are crucial for preparing text data for embedding and vector storage:\n",
    "\n",
    "1. They ensure consistency in the data.\n",
    "2. They reduce noise and focus on meaningful content.\n",
    "3. They normalize the text, making it easier for embedding algorithms to capture semantic relationships.\n",
    "4. They can improve the efficiency and effectiveness of subsequent NLP tasks.\n",
    "\n",
    "The specific combination and order of these techniques may vary depending on the particular requirements of your embedding method and the characteristics of your dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
